{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454a06a",
   "metadata": {},
   "source": [
    "## HDAT 9910 Capstone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be208044",
   "metadata": {},
   "source": [
    "Research Question 1: Mortality Prediction in ICU \n",
    "\n",
    "Task: The task is to build a predictive algorithm using the techniques we learned in this course. \n",
    "\n",
    "Objective: To assess the role of machine learning algorithms for predicting mortality by using the MIMIC-III dataset. \n",
    "\n",
    "Question: Is it possible to accurately predict mortality based on data from the first 24 hours in ICU?   \n",
    "\n",
    "Study Population: MIMIC-III dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5d12e",
   "metadata": {},
   "source": [
    "#### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "14c0c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import concurrent.futures\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243967cf",
   "metadata": {},
   "source": [
    "#### Create a function to loads through all CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "0577bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_datasets(data_folder):\n",
    "    \n",
    "    datasets = {}\n",
    "\n",
    "    csv_files = [\n",
    "        'vitals_hourly.csv', 'admissions.csv', 'antibiotics.csv', 'bloodculture.csv',\n",
    "        'gcs_hourly.csv', 'icd9_diag.csv', 'icustays.csv', 'labs_hourly.csv',\n",
    "        'output_hourly.csv', 'patients.csv', 'pt_icu_outcome.csv', 'pt_stay_hr.csv',\n",
    "        'pt_weight.csv', 'pv_mechvent.csv', 'transfers.csv', 'vasopressors.csv'\n",
    "    ]\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            datasets[file.replace('.csv', '')] = pd.read_csv(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "data_folder = '/Users\\lukac\\OneDrive\\Desktop\\HDAT-9910-Capstone/mimic_data/'\n",
    "all_datasets = read_all_datasets(data_folder)\n",
    "\n",
    "# Assign all files to a dataframe for exploration\n",
    "admissions_df = all_datasets['admissions']\n",
    "vitals_hourly_df = all_datasets['vitals_hourly']\n",
    "antibiotics_df = all_datasets['antibiotics']\n",
    "bloodculture_df = all_datasets['bloodculture']\n",
    "gcs_hourly_df = all_datasets['gcs_hourly']\n",
    "icd9_diag_df = all_datasets['icd9_diag']\n",
    "icustays_df = all_datasets['icustays']\n",
    "labs_hourly_df = all_datasets['labs_hourly']\n",
    "pt_stay_hr_df = all_datasets['pt_stay_hr']\n",
    "pt_icu_outcome_df = all_datasets['pt_icu_outcome']\n",
    "patients_df = all_datasets['patients']\n",
    "output_hourly_df = all_datasets['output_hourly']\n",
    "pt_weight_df = all_datasets['pt_weight']\n",
    "pv_mechvent_df = all_datasets['pv_mechvent']\n",
    "transfers_df = all_datasets['transfers']\n",
    "vasopressors_df = all_datasets['vasopressors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "42b582a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in admissions_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    admissions_df[col].fillna(admissions_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in admissions_df.select_dtypes(include=['object']).columns:\n",
    "    admissions_df[col].fillna(admissions_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "admissions_df.drop_duplicates(inplace=True)\n",
    "admissions_df = admissions_df.drop(columns =[\n",
    "    'hadm_id', 'marital_status','religion','language',\n",
    "    'ethnicity','dischtime','edregtime', \n",
    "     'hospital_expire_flag','has_chartevents_data',\n",
    "     'row_id','edouttime', 'discharge_location','deathtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "29c7139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in patients_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    patients_df[col].fillna(patients_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in patients_df.select_dtypes(include=['object']).columns:\n",
    "    patients_df[col].fillna(patients_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "patients_df.drop_duplicates(inplace=True)\n",
    "patients_df = patients_df.drop(columns =[\n",
    "    'dod_ssn', 'dob','dod_hosp', 'row_id', 'dod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "60efa920",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in vitals_hourly_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    vitals_hourly_df[col].fillna(vitals_hourly_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in vitals_hourly_df.select_dtypes(include=['object']).columns:\n",
    "    vitals_hourly_df[col].fillna(vitals_hourly_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "vitals_hourly_df.drop_duplicates(inplace=True)\n",
    "vitals_hourly_df = vitals_hourly_df.drop(columns =['fio2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d1511f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in gcs_hourly_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    gcs_hourly_df[col].fillna(gcs_hourly_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in gcs_hourly_df.select_dtypes(include=['object']).columns:\n",
    "    gcs_hourly_df[col].fillna(gcs_hourly_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "gcs_hourly_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "70f8d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in labs_hourly_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    labs_hourly_df[col].fillna(labs_hourly_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in labs_hourly_df.select_dtypes(include=['object']).columns:\n",
    "    labs_hourly_df[col].fillna(labs_hourly_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "labs_hourly_df.drop_duplicates(inplace=True)\n",
    "labs_hourly_df = labs_hourly_df.drop(columns =[\n",
    "    'alaninetransaminase', 'aspartatetransaminase', 'albumin', 'bilirubin','glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "7d5011b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in pt_icu_outcome_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    pt_icu_outcome_df[col].fillna(pt_icu_outcome_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in pt_icu_outcome_df.select_dtypes(include=['object']).columns:\n",
    "    pt_icu_outcome_df[col].fillna(pt_icu_outcome_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "pt_icu_outcome_df.drop_duplicates(inplace=True)\n",
    "pt_icu_outcome_df = pt_icu_outcome_df.drop(columns =[\n",
    "    'hadm_id', 'icu_expire_flag', 'row_id','dob', 'admittime', 'dischtime',\n",
    "    'ttd_days','dod','hospital_expire_flag', 'intime', 'los','hosp_deathtime',\n",
    "    'outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "9145239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill missing values with the median\n",
    "for col in icustays_df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    icustays_df[col].fillna(icustays_df[col].median(), inplace=True)\n",
    "\n",
    "# For categorical columns, fill missing values with the mode\n",
    "for col in icustays_df.select_dtypes(include=['object']).columns:\n",
    "    icustays_df[col].fillna(icustays_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "icustays_df.drop_duplicates(inplace=True)\n",
    "icustays_df = icustays_df.drop(columns =[\n",
    "    'row_id', 'dbsource', 'hadm_id','first_careunit', 'last_careunit', \n",
    "    'first_wardid','last_wardid','intime','outtime', 'intime', 'los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "f0ebdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records within the first 24 hours\n",
    "vitals_first_24h = vitals_hourly_df[vitals_hourly_df['hr'] <= 24]\n",
    "vitals_first_24h = vitals_first_24h[vitals_hourly_df['hr'] >=0]\n",
    "# Aggregate the data (mean, min, max)\n",
    "vitals_agg = vitals_first_24h.groupby('icustay_id').agg(['mean', 'min', 'max']).reset_index()\n",
    "\n",
    "# Adjust the column names in the aggregated DataFrame\n",
    "vitals_agg.columns = ['_'.join(col).rstrip('_') if col[0] != 'icustay_id' else col[0] for col in vitals_agg.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "91d248e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records within the first 24 hours\n",
    "labs_first_24h = labs_hourly_df[labs_hourly_df['hr'] <= 24]\n",
    "labs_first_24h = labs_first_24h[labs_first_24h['hr'] >=0]\n",
    "# Aggregate the data (mean, min, max)\n",
    "labs_agg = labs_first_24h.groupby('icustay_id').agg(['mean', 'min', 'max']).reset_index()\n",
    "\n",
    "# Adjust the column names in the aggregated DataFrame\n",
    "labs_agg.columns = ['_'.join(col).rstrip('_') if col[0] != 'icustay_id' else col[0] for col in labs_agg.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "9b521f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records within the first 24 hours\n",
    "gcs_first_24h = gcs_hourly_df[gcs_hourly_df['hr'] <= 24]\n",
    "gcs_first_24h = gcs_first_24h[gcs_first_24h['hr'] >= 0]\n",
    "\n",
    "# Aggregate the data (mean, min, max)\n",
    "gcs_agg = gcs_first_24h.groupby('icustay_id').agg(['mean', 'min','max']).reset_index()\n",
    "\n",
    "# Adjust the column names in the aggregated DataFrame\n",
    "gcs_agg.columns = ['_'.join(col).rstrip('_') if col[0] != 'icustay_id' else col[0] for col in gcs_agg.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "76396dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate admissions_df to get the earliest admission for each subject_id\n",
    "aggregated_admissions = admissions_df.sort_values(by=['subject_id', 'admittime']).drop_duplicates('subject_id', keep='last')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "030ea870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patients and ICU stays\n",
    "merged_df = pd.merge(icustays_df, aggregated_admissions, on='subject_id', how='inner')\n",
    "merged_df = pd.merge(merged_df, patients_df, on='subject_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "d86c4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['icustay_id'] = merged_df['icustay_id'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "fc786f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in aggregated vitals, labs, and GCS data\n",
    "merged_df = pd.merge(merged_df, vitals_agg, on='icustay_id', how='left')\n",
    "merged_df = pd.merge(merged_df, labs_agg, on='icustay_id', how='left')  \n",
    "merged_df = pd.merge(merged_df, gcs_agg, on='icustay_id', how='left')  \n",
    "# Merge in ICU outcomes\n",
    "merged_df = pd.merge(merged_df, pt_icu_outcome_df, on='icustay_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "e13224f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not of importance\n",
    "merged_df = merged_df.drop(columns =[\n",
    "    'subject_id_x','hr_mean_x', 'hr_min_x','hr_max_x', 'hr_mean_y', 'hr_min_y',\n",
    "    'hr_max_y','subject_id_y','expire_flag_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "ebc513a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'gender' to bindary, 1 for 'M' and 0 for 'F'\n",
    "merged_df['gender'] = merged_df['gender'].map({'M': 1, 'F': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "bc0b5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easy interpretation\n",
    "merged_df.rename(columns={'expire_flag_x': 'mortality', 'age_years'  : 'age'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "65d67215",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_weeked = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5e2b2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = merged_df.select_dtypes(include=['object', 'category']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "a2f7ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_and_encode_top_categories(df, col, top_n=300):\n",
    "    # Find the top n most frequent categories in the column\n",
    "    top_categories = df[col].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Function to replace categories not in the top n with 'Other'\n",
    "    def reduce_categories(val):\n",
    "        if val not in top_categories:\n",
    "            return 'Other'\n",
    "        return val\n",
    "    \n",
    "    # Reduce the categories in the column\n",
    "    reduced_col_name = f'reduced_{col}'\n",
    "    df[reduced_col_name] = df[col].apply(reduce_categories)\n",
    "    \n",
    "    # Apply one-hot encoding to the reduced category column\n",
    "    df_encoded = pd.get_dummies(df, columns=[reduced_col_name], prefix=reduced_col_name)\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "dc66d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_process = ['admission_type', 'admission_location', 'insurance','diagnosis']\n",
    "for col in columns_to_process:\n",
    "    merged_df = reduce_and_encode_top_categories(merged_df, col, top_n=300) \n",
    "merged_df.drop(['diagnosis', 'admission_type','admission_location','insurance'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "4f738c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'admit_time' to datetime\n",
    "merged_df_weeked['admittime'] = pd.to_datetime(merged_df_weeked['admittime'])\n",
    "\n",
    "# Extract day of week from 'admit_time' (0=Monday, 6=Sunday)\n",
    "merged_df_weeked['day_of_week'] = merged_df_weeked['admittime'].dt.dayofweek\n",
    "\n",
    "# Identify weekend admissions (Saturday=5, Sunday=6)\n",
    "merged_df_weeked['weekend_admission'] = merged_df_weeked['day_of_week'].apply(lambda x: 1 if x in [5, 6] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "af453e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.drop('admittime',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "37fb3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_weeked.drop(['admittime','day_of_week'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "2b390a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe so we can load it into our other scripts\n",
    "output_csv_path = '/Users\\lukac\\OneDrive\\Desktop\\HDAT-9910-Capstone/df_24Hrs.csv'\n",
    "merged_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "75cf8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe so we can load it into our other scripts\n",
    "#output_csv_path = '/Users\\lukac\\OneDrive\\Desktop\\HDAT-9910-Capstone/df_weekend.csv'\n",
    "#merged_df_weeked.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "0b1e31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe so we can load it into our other scripts\n",
    "output_csv_path = '/Users\\lukac\\OneDrive\\Desktop\\HDAT-9910-Capstone/df_weekend_R.csv'\n",
    "merged_df_weeked.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44616526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
